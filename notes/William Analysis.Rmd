---
title: "Exact Asymptotics for Causal Mediation Analysis"
author: "William Ruth"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
# Force text wrapping for long lines
library(formatR)
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)

library(kableExtra)


knitr::opts_chunk$set(echo = TRUE)

```

I'm doing some Monte Carlo to verify the new SE formulas. Recall that we're doing mediation analysis, so we've got a response, $Y$, an exposure, $X$, and a mediation, $M$. We also have some number of confounders, which will be grouped together in the matrix $W$. Broadly speaking, we fit two regression models, one to predict $M$ using $X$ and $W$, the other to predict $Y$ using $M$, $X$ and $W$. We then compute the mediation effect (specifically, the total effect of $X$ on $Y$) as a function of the coefficients from these two regression models. An asymptotic SE for our mediation effect estimator can then be obtained from the asymptotic standard errors of our fitted regression coefficients using the $\delta$-method.

So far, so simple. There are a few places that things start to get more complicated. First, each of the two regression models can be either linear of logistic depending on whether the corresponding response variable is continuous or binary\footnote{In principle, we could have $Y$ and/or $M$ follow any distribution with a suitable GLM formulation. I don't think I've ever seen count data (i.e. Poisson regression) used here, much less anything more exotic.}. Furthermore, we can add random effects to our regression models. In the trust study, we have random effects for the intercept, $X$ and $M$ (naturally, the latter only applies when predicting $Y$).

In each setting, I generate $X$ and three confounders, $W = [W_1, W_2, W_3]$, each iid $\mathrm{N}(0,1)$. In both models, I set regression slopes to 1, and choose the intercept so that the mean of the linear predictor is approximately zero\footnote{In the fully continuous case, the mean linear predictor is exactly zero. When the mediator is binary, I choose the $Y$-intecept as though $\mathbb{E}M=\mathrm{expit}(0)=0.5$}. I generate some datasets (typically 1000, can be adjusted as necessary) with each of $n=100, 1000$ and $10000$ observations. On each dataset, we estimate the mediation effect and its SE (using the $\delta$-method). We then compare the empirical SE (i.e. SD over the $M$ Monte Carlo replicates) to the mean and median of our $M$ estimated SEs.

In summary, we can setup each model with the same boilerplate code as follows:

```{r, boilerplate}
# , cached=TRUE}
# Note: This code chunk is not run here, but it is called and run at the start of every subsequent section. Any changes made here will be duplicated throughout the rest of the document.

# set.seed(1)
set.seed(12345)

num_reps = 1000

all_Ns = c(100, 200, 500, 1000, 2000, 5000, 10000)
p_conf = 3  # Number of confounders


# Regression coefficients
## Intercepts are set separately for each model so that the mean of the linear predictor is approximately zero

## M model
a_1 = 1                       # Coefficient for X
A_2 = rep(1, times = p_conf)  # Coefficients for confounders

# Y model
b_1 = 1                       # Coefficient for M
b_2 = 1                       # Coefficient for X
B_3 = rep(1, times = p_conf)  # Coefficients for confounders


# Containers for output
all_a_hats = list()
all_b_hats = list()

all_a_SEs = list()
all_b_SEs = list()

all_med_hats = list()
all_med_SEs = list()


```

In the rest of this document, we carry our Monte Carlo studies for different model configurations. We start with the simplest case: continuous response, continuous mediator, and fixed-effects. We then move to a binary mediator, followed by binary response. Finally, we introduce random effects.

# Continuous Response, Continuous Mediator, Fixed-Effects

```{r, include=FALSE}
# Run the boilerplate setup
<<boilerplate>>
```

## Generate data

Continuous variables are modelled as linear predictor plus residual. We therefore need to set the residual variance.

```{r}
sigma_M = 0.2
sigma_Y = 0.2
```

We also need to set the intercepts for the two models.
```{r}
a_0 = 0
b_0 = 0
```

For pedagogical purposes, I will demonstrate the analysis on a single simulated dataset, then run the full MC study invisibly and only show the results. First, we choose a sample size, $n$, and generate $X$ and $W$, then use them to generate $M$ and $Y$.

```{r}
n = all_Ns[1]

X = rnorm(n, mean=0, sd=1)
W = matrix(rnorm(n*p_conf, mean=0, sd=1), nrow = n, ncol = p_conf)

# Generate M
e_M = rnorm(n, 0, sigma_M)
M = a_0 + a_1*X + W%*%A_2 + e_M

# Generate Y
e_Y = rnorm(n, 0, sigma_Y)
Y = b_0 + b_1*M + b_2*X + W%*%B_3 + e_Y
```

## Estimate Mediation Effect

Next, we fit regression models for $M$ and $Y$ and extract relevant output (in the loop version, these are stored at each iteration)

```{r}
M_data = data.frame(M, X, W1 = W[,1], W2 = W[,2], W3 = W[,3])
M_model = lm(M ~ X + W1 + W2 + W3, data = M_data)

a_hat = summary(M_model)$coefficients[,1]
a_SE = summary(M_model)$coefficients[,2]
a_cov = vcov(M_model)


Y_data = data.frame(Y, M, X, W1 = W[,1], W2 = W[,2], W3 = W[,3])
Y_model = lm(Y ~ M + X + W1 + W2 + W3, data = Y_data)

b_hat = summary(Y_model)$coefficients[,1]
b_SE = summary(Y_model)$coefficients[,2]
b_cov = vcov(Y_model)
```

Now we can extract relevant coefficients and estimate the mediation effect

```{r}
a_x = a_hat[2]
b_x = b_hat[3]
b_m = b_hat[2]

med_hat = a_x*b_m + b_x
```

## Estimate Standard Error

Finally, we can estimate the SE of the mediation effect using the $\delta$-method. The way I do this is excessive here since the mediation effect has such a simple formula. Later though, the extra structure will make our lives easier.

We start by constructing the joint covariance matrix of the regression coefficients from the two models. This matrix is block-diagonal, with the blocks corresponding to the covariance matrices of the coefficients from the two models and off-diagonal entries zero.

```{r}
a_length = nrow(a_cov)
b_length = nrow(b_cov)

joint_cov = matrix(0, nrow = a_length + b_length, ncol = a_length + b_length)
joint_cov[1:a_length, 1:a_length] = a_cov
joint_cov[(a_length+1):(a_length+b_length), (a_length+1):(a_length+b_length)] = b_cov
```

Next, we compute the gradient of the mediation effect with respect to each regression coefficient.

```{r}
grad_a_0 = 0
grad_a_1 = b_m
grad_A_2 = rep(0, times = p_conf)

grad_b_0 = 0
grad_b_1 = a_x
grad_b_2 = 1
grad_B_3 = rep(0, times = p_conf)

grad_med = c(grad_a_0, grad_a_1, grad_A_2, grad_b_0, grad_b_1, grad_b_2, grad_B_3)
```

Finally, we can use the delta method to estimate the SE of the mediation effect. Note that the $\delta$-method works on asymptotic covariances, so we need to make sure to multiply/divide by $n$ or $\sqrt{n}$ where appropriate.

```{r}
asymp_reg_cov = n * joint_cov

med_asymp_var = grad_med %*% asymp_reg_cov %*% grad_med
med_asymp_SE = sqrt(med_asymp_var)

med_SE = med_asymp_SE / sqrt(n)
```

Putting everything together, we have an estimated mediation effect of `r med_hat` with an estimated SE of `r med_SE`.


## Monte Carlo Study

We now repeat the above analysis `r num_reps` times for each of various values of $n$. We also re-run the boilerplate code (not shown)

```{r, include=FALSE}
<<boilerplate>>
```


```{r MC-cont-cont-fix, include=FALSE, cache=TRUE}
# , dependson=c("boilerplate")}

for(j in seq_along(all_Ns)){
  n = all_Ns[j]
  
  # Containers for output with this value of n
  some_a_hats = list()
  some_b_hats = list()
  
  some_a_SEs = list()
  some_b_SEs = list()
  
  some_med_hats = list()
  some_med_SEs = list()
  
  for(i in 1:num_reps){
    # Generate data
    X = rnorm(n, mean=0, sd=1)
    W = matrix(rnorm(n*p_conf, mean=0, sd=1), nrow = n, ncol = p_conf)
    
    ## Generate M
    e_M = rnorm(n, 0, sigma_M)
    M = a_0 + a_1*X + W%*%A_2 + e_M
    
    ## Generate Y
    e_Y = rnorm(n, 0, sigma_Y)
    Y = b_0 + b_1*M + b_2*X + W%*%B_3 + e_Y
    
    
    # Fit models
    
    ## M
    M_data = data.frame(M, X, W1 = W[,1], W2 = W[,2], W3 = W[,3])
    M_model = lm(M ~ X + W1 + W2 + W3, data = M_data)
    
    a_hat = summary(M_model)$coefficients[,1]
    a_SE = summary(M_model)$coefficients[,2]
    a_cov = vcov(M_model)
    
    some_a_hats[[i]] = a_hat
    some_a_SEs[[i]] = a_SE
    
    
    ## Y
    Y_data = data.frame(Y, M, X, W1 = W[,1], W2 = W[,2], W3 = W[,3])
    Y_model = lm(Y ~ M + X + W1 + W2 + W3, data = Y_data)
    
    b_hat = summary(Y_model)$coefficients[,1]
    b_SE = summary(Y_model)$coefficients[,2]
    b_cov = vcov(Y_model)
    
    some_b_hats[[i]] = b_hat
    some_b_SEs[[i]] = b_SE
    
    
    # Estimate mediation effect
    
    ## Extract coefficients
    a_x = a_hat[2]
    b_x = b_hat[3]
    b_m = b_hat[2]
    
    ## Compute estimate
    med_hat = a_x*b_m + b_x
    some_med_hats[[i]] = med_hat
    
    
    # Estimate SE
    
    ## Build joint covariance matrix
    a_length = nrow(a_cov)
    b_length = nrow(b_cov)
    
    joint_cov = matrix(0, nrow = a_length + b_length, ncol = a_length + b_length)
    joint_cov[1:a_length, 1:a_length] = a_cov
    joint_cov[(a_length+1):(a_length+b_length), (a_length+1):(a_length+b_length)] = b_cov
    
    
    ## Compute gradient of mediation effect
    grad_a_0 = 0
    grad_a_1 = b_m
    grad_A_2 = rep(0, times = p_conf)
    
    grad_b_0 = 0
    grad_b_1 = a_x
    grad_b_2 = 1
    grad_B_3 = rep(0, times = p_conf)
    
    grad_med = c(grad_a_0, grad_a_1, grad_A_2, grad_b_0, grad_b_1, grad_b_2, grad_B_3)
    
    
    ## Apply delta-method
    asymp_reg_cov = n * joint_cov

    med_asymp_var = grad_med %*% asymp_reg_cov %*% grad_med
    med_asymp_SE = sqrt(med_asymp_var)
    
    med_SE = med_asymp_SE / sqrt(n)
    some_med_SEs[[i]] = med_SE
  }
  
  # Store output for current value of n
  all_a_hats[[j]] = some_a_hats
  all_b_hats[[j]] = some_b_hats
  
  all_a_SEs[[j]] = some_a_SEs
  all_b_SEs[[j]] = some_b_SEs
  
  all_med_hats[[j]] = some_med_hats
  all_med_SEs[[j]] = some_med_SEs
}

```

We now process the output from our simulation and summarize the results in a table. We give the mean and median of the estimated standard errors, as well as their relative errors (in $\%$) as estimates of the empirical standard error. See Table \ref{tab:results_cont_cont_fix}. Note that the relative error decreased and became more consistent when I increased the number of Monte Carlo replicates from 100 to 1000.

```{r}
# Re-format output to be more useful. 
data_med_hats = lapply(all_med_hats, unlist)
data_med_SEs = lapply(all_med_SEs, unlist)

# SD of estimates across Monte Carlo samples
emp_SEs = sapply(data_med_hats, sd)

# Summaries of estimated SEs (mean and median)
mean_delta_SEs = sapply(data_med_SEs, mean)
median_delta_SEs = sapply(data_med_SEs, median)


# Combine into a table
results = data.frame(n = all_Ns, Empirical = emp_SEs, 
                     Mean = mean_delta_SEs, Median = median_delta_SEs,
                     "Percent_Err_Mean" = 100*(mean_delta_SEs - emp_SEs)/emp_SEs,
                     "Percent_Err_Median" = 100*(median_delta_SEs - emp_SEs)/emp_SEs)

```

```{r results="asis", echo=FALSE}
## Print table
kbl(results, format = "latex", booktabs = T, caption = "Summary of SE estimates for mediation effect under continuous response, continuous mediator, fixed-effects model \\label{tab:results_cont_cont_fix}")
```







# Continuous Response, Binary Mediator, Fixed-Effects

```{r, include=FALSE}
# Run the boilerplate setup
<<boilerplate>>
```

We now generate $M$ as a binary variable using a logistic regression model. We continue to use a continuous model for $Y|M$. This analysis is sufficiently complicated that I wrote some helper functions in another script. See `Helpers.R`.

```{r}
source("../src/Helpers.R")
```


## Generate data

The continuous variable is modelled as linear predictor plus residual. We therefore need to set the residual variance.

```{r}
sigma_Y = 0.2
```

We also need to set the intercepts for the two models. Recall that we want the mean of the linear predictor to be zero in both models, and that we approximate the mean of $M$ as the logit of the mean of its linear predictor (i.e. $\mathbb{E}M \approx \mathrm{expit}(0) = 0.5$).
```{r}
a_0 = 0
b_0 = -0.5
```

As in the previous section, I demonstrate the analysis on a single simulated dataset, then run the full MC study invisibly and only show the results. First, we choose a sample size, $n$, and generate $X$ and $W$, then use them to generate $M$ and $Y$.

```{r}
n = all_Ns[1]

X = rnorm(n, mean=0, sd=1)
W = matrix(rnorm(n*p_conf, mean=0, sd=1), nrow = n, ncol = p_conf)

# Generate M
eta_vec = a_0 + a_1*X + W%*%A_2
p_M_vec = expit(eta_vec)
M = rbinom(n, size = 1, prob = p_M_vec)

# Generate Y
e_Y = rnorm(n, 0, sigma_Y)
Y = b_0 + b_1*M + b_2*X + W%*%B_3 + e_Y
```

## Estimate Mediation Effect

Next, we fit regression models for $M$ and $Y$ and extract relevant output (in the loop version, these are stored at each iteration)

```{r}
M_data = data.frame(M, X, W1 = W[,1], W2 = W[,2], W3 = W[,3])
M_model = glm(M ~ X + W1 + W2 + W3, data = M_data, family = binomial(link = "logit"))

a_hat = summary(M_model)$coefficients[,1]
a_SE = summary(M_model)$coefficients[,2]
a_cov = vcov(M_model)


Y_data = data.frame(Y, M, X, W1 = W[,1], W2 = W[,2], W3 = W[,3])
Y_model = lm(Y ~ M + X + W1 + W2 + W3, data = Y_data)

b_hat = summary(Y_model)$coefficients[,1]
b_SE = summary(Y_model)$coefficients[,2]
b_cov = vcov(Y_model)
```

Now we can extract relevant coefficients and estimate the mediation effect. Note that this is more involved than the fully continuous case. In particular, the total effect of $X$ on $Y$ depends on the levels of $X$ and the confounders, $W$. I evaluate the effect at $X=0$ and $W = [1,1,1]$. The former represents the effect of a binary exposure, but the latter is chosen completely arbitrarily.

```{r}
x_pred = 0
W_pred = c(1,1,1)

a_0_hat = a_hat[1]
a_x_hat = a_hat[2]
A_2_hat = a_hat[3:5]

b_0_hat = b_hat[1]
b_m_hat = b_hat[2]
b_x_hat = b_hat[3]
B_3_hat = b_hat[4:6]

# Linear predictor for M
eta_hat = a_0_hat + a_x_hat*x_pred + W_pred%*%A_2_hat

# Increment in the conditional expectation of M
delta_hat = get_delta(eta_hat, a_x_hat)

# Increment in the conditional expectation of Y
med_hat = get_gamma(delta_hat, b_m_hat, b_x_hat)
```


## Estimate Standard Error

Finally, we can estimate the SE of the mediation effect using the $\delta$-method. Now that we have a more complicated expression for the total effect, it makes more sense to use the general formula for the $\delta$-method standard error.

We start by constructing the joint covariance matrix of the regression coefficients from the two models. This matrix is block-diagonal, with the blocks corresponding to the covariance matrices of the coefficients from the two models and off-diagonal entries zero.

```{r}
a_length = nrow(a_cov)
b_length = nrow(b_cov)

joint_cov = matrix(0, nrow = a_length + b_length, ncol = a_length + b_length)
joint_cov[1:a_length, 1:a_length] = a_cov
joint_cov[(a_length+1):(a_length+b_length), (a_length+1):(a_length+b_length)] = b_cov
```

Next, we compute the gradient of the mediation effect with respect to each regression coefficient.

```{r}
d_gamma_d_theta(eta_hat, x_pred, W_pred, a_hat, b_hat)
```

Finally, we can use the delta method to estimate the SE of the mediation effect. Note that the $\delta$-method works on asymptotic covariances, so we need to make sure to multiply/divide by $n$ or $\sqrt{n}$ where appropriate.

```{r}
asymp_reg_cov = n * joint_cov

med_asymp_var = grad_med %*% asymp_reg_cov %*% grad_med
med_asymp_SE = sqrt(med_asymp_var)

med_SE = med_asymp_SE / sqrt(n)
```

Putting everything together, we have an estimated mediation effect of `r med_hat` with an estimated SE of `r med_SE`.


## Monte Carlo Study

We now repeat the above analysis `r num_reps` times for each of various values of $n$. Values of $X$ and $W$ for which we compute the total effect are set at the beginning. We also re-run the boilerplate code (not shown)

```{r, include=FALSE}
<<boilerplate>>
```

```{r}
# Values of X and W for which we compute the total effect
x_pred = 0
W_pred = c(1,1,1)
```



```{r MC-cont-bin-fix, include=FALSE, cache=TRUE}
# , dependson=c("boilerplate")}

for(j in seq_along(all_Ns)){
  n = all_Ns[j]
  
  # Containers for output with this value of n
  some_a_hats = list()
  some_b_hats = list()
  
  some_a_SEs = list()
  some_b_SEs = list()
  
  some_med_hats = list()
  some_med_SEs = list()
  
  for(i in 1:num_reps){
    # Generate data
    X = rnorm(n, mean=0, sd=1)
    W = matrix(rnorm(n*p_conf, mean=0, sd=1), nrow = n, ncol = p_conf)
    
    ## Generate M
    eta_vec = a_0 + a_1*X + W%*%A_2
    p_M_vec = expit(eta_vec)
    M = rbinom(n, size = 1, prob = p_M_vec)
    
    ## Generate Y
    e_Y = rnorm(n, 0, sigma_Y)
    Y = b_0 + b_1*M + b_2*X + W%*%B_3 + e_Y
    
    
    # Fit models
    
    ## M
    M_data = data.frame(M, X, W1 = W[,1], W2 = W[,2], W3 = W[,3])
    M_model = glm(M ~ X + W1 + W2 + W3, data = M_data, family = binomial(link = "logit"))
    
    a_hat = summary(M_model)$coefficients[,1]
    a_SE = summary(M_model)$coefficients[,2]
    a_cov = vcov(M_model)
    
    some_a_hats[[i]] = a_hat
    some_a_SEs[[i]] = a_SE
    
    
    ## Y
    Y_data = data.frame(Y, M, X, W1 = W[,1], W2 = W[,2], W3 = W[,3])
    Y_model = lm(Y ~ M + X + W1 + W2 + W3, data = Y_data)
    
    b_hat = summary(Y_model)$coefficients[,1]
    b_SE = summary(Y_model)$coefficients[,2]
    b_cov = vcov(Y_model)
    
    some_b_hats[[i]] = b_hat
    some_b_SEs[[i]] = b_SE
    
    
    # Estimate mediation effect
    
    ## Extract coefficients
    a_0_hat = a_hat[1]
    a_x_hat = a_hat[2]
    A_2_hat = a_hat[3:length(a_hat)]
    
    b_0_hat = b_hat[1]
    b_m_hat = b_hat[2]
    b_x_hat = b_hat[3]
    B_3_hat = b_hat[4:length(b_hat)]
    
    ## Linear predictor for M
    eta_hat = a_0_hat + a_x_hat*x_pred + W_pred%*%A_2_hat

    ## Increment in the conditional expectation of M
    delta_hat = get_delta(eta_hat, a_x_hat)

    ## Increment in the conditional expectation of Y (i.e. mediation effect)
    med_hat = get_gamma(delta_hat, b_m_hat, b_x_hat)
    some_med_hats[[i]] = med_hat
    
    
    # Estimate SE
    
    ## Build joint covariance matrix
    a_length = nrow(a_cov)
    b_length = nrow(b_cov)
    
    joint_cov = matrix(0, nrow = a_length + b_length, ncol = a_length + b_length)
    joint_cov[1:a_length, 1:a_length] = a_cov
    joint_cov[(a_length+1):(a_length+b_length), (a_length+1):(a_length+b_length)] = b_cov
    
    
    ## Compute gradient of mediation effect
    grad_med = d_gamma_d_theta(eta_hat, x_pred, W_pred, a_hat, b_hat)
    
    
    ## Apply delta-method
    asymp_reg_cov = n * joint_cov

    med_asymp_var = grad_med %*% asymp_reg_cov %*% grad_med
    med_asymp_SE = sqrt(med_asymp_var)
    
    med_SE = med_asymp_SE / sqrt(n)
    some_med_SEs[[i]] = med_SE
  }
  
  # Store output for current value of n
  all_a_hats[[j]] = some_a_hats
  all_b_hats[[j]] = some_b_hats
  
  all_a_SEs[[j]] = some_a_SEs
  all_b_SEs[[j]] = some_b_SEs
  
  all_med_hats[[j]] = some_med_hats
  all_med_SEs[[j]] = some_med_SEs
}

```

We now process the output from our simulation and summarize the results in a table. We give the mean and median of the estimated standard errors, as well as their relative errors (in $\%$) as estimates of the empirical standard error. See Table \ref{tab:results_cont_bin_fix}. Note that the relative error decreased and became more consistent when I increased the number of Monte Carlo replicates from 100 to 1000.

```{r}
# Re-format output to be more useful. 
data_med_hats = lapply(all_med_hats, unlist)
data_med_SEs = lapply(all_med_SEs, unlist)

# SD of estimates across Monte Carlo samples
emp_SEs = sapply(data_med_hats, sd)

# Summaries of estimated SEs (mean and median)
mean_delta_SEs = sapply(data_med_SEs, mean)
median_delta_SEs = sapply(data_med_SEs, median)


# Combine into a table
results = data.frame(n = all_Ns, Empirical = emp_SEs, 
                     Mean = mean_delta_SEs, Median = median_delta_SEs,
                     "Percent_Err_Mean" = 100*(mean_delta_SEs - emp_SEs)/emp_SEs,
                     "Percent_Err_Median" = 100*(median_delta_SEs - emp_SEs)/emp_SEs)

```

```{r results="asis", echo=FALSE}
## Print table
kbl(results, format = "latex", booktabs = T, caption = "Summary of SE estimates for mediation effect under continuous response, continuous mediator, fixed-effects model \\label{tab:results_cont_bin_fix}")
```



# Binary Response, Binary Mediator, Fixed-Effects

For our last fixed-effect model, we now take both the response and mediator to be binary. I wrote some helper functions to compute relevant derivatives, all of which validated against a simple finite-difference approximation. See `Helpers.R`.

```{r}
source("../src/Helpers.R")
```

## Generate Data

As in the previous case, we set intercepts so that the means of both linear predictors are at least approximately zero. Here however, no residuals are required.

```{r}
a_0 = 0
b_0 = -0.5
```

We first demonstrate our analysis on a single dataset, then put this analysis inside a `for` loop. The first step is to set the sample size and generate some data. We use $\eta$ to denote the linear predictor for $M$, and $\zeta$ for the linear predictor of $Y$. <!-- (This is a comment) without the $M$ term. That is, $\zeta = b_0 + b_2 X + W B_3$. -->


```{r}
n = all_Ns[1]

X = rnorm(n, mean=0, sd=1)
W = matrix(rnorm(n*p_conf, mean=0, sd=1), nrow = n, ncol = p_conf)

# Generate M
eta_vec = a_0 + a_1*X + W%*%A_2
p_M_vec = expit(eta_vec)
M = rbinom(n, size = 1, prob = p_M_vec)

# Generate Y
zeta_vec = b_0 + b_1*M + b_2*X + W%*%B_3
p_Y_vec = expit(zeta_vec)
Y = rbinom(n, size = 1, prob = p_Y_vec)
```


## Estimate Mediation Effect

We now fit logistic regression models to predict $M$ and $Y$, and extract output that we will need later.

```{r}
M_data = data.frame(M, X, W1 = W[, 1], W2 = W[, 2], W3 = W[,3])
M_model = glm(M ~ X + W1 + W2 + W3, data = M_data, family = binomial(link = "logit"))

a_hat = summary(M_model)$coefficients[, 1]
a_SE = summary(M_model)$coefficients[, 2]
a_cov = vcov(M_model)


Y_data = data.frame(Y, M, X, W1 = W[, 1], W2 = W[, 2], W3 = W[,3])
Y_model = glm(Y ~ M + X + W1 + W2 + W3, data = Y_data, family = binomial(link = "logit"))

b_hat = summary(Y_model)$coefficients[, 1]
b_SE = summary(Y_model)$coefficients[, 2]
b_cov = vcov(Y_model)
```

Next, we estimate the mediation effect. Note that, as in the contiuous outcome, binary mediator setting, our mediation effect depends on $X$ and $W$. We again use $X=0$ and $W=[1,1,1]$, although Samoilenko and Lefebvre (2023) use the sample means of these covariates.

```{r}
x_pred = 0
W_pred = c(1,1,1)

a_0_hat = a_hat[1]
a_x_hat = a_hat[2]
A_2_hat = a_hat[3:5]

b_0_hat = b_hat[1]
b_m_hat = b_hat[2]
b_x_hat = b_hat[3]
B_3_hat = b_hat[4:6]

# Linear predictors
eta_hat = a_0_hat + a_x_hat*x_pred + W_pred%*%A_2_hat
zeta_hat = b_0_hat + b_x_hat * x_pred + W_pred %*% B_3_hat

# Mediation effect
## See Helpers.R for the function get_odds_ratio
med_hat = get_odds_ratio(eta_hat, a_x, zeta_hat, b_x, b_m)
```

We now compute the gradient of the odds ratio and use the $\delta$-method to get the standard error.

```{r}
# Gradient of OR wrt regression coefficients
grad_med = d_OR_d_theta(eta_hat, a_x_hat, zeta_hat, b_x_hat, b_m_hat, x_pred, W_pred)


# Get asymptotic SE using delta method

## Build joint covariance matrix of regression coefficients
a_length = nrow(a_cov)
b_length = nrow(b_cov)
joint_cov = matrix(0, nrow = a_length + b_length, ncol = a_length +
b_length)
joint_cov[1:a_length, 1:a_length] = a_cov
joint_cov[(a_length + 1):(a_length + b_length), (a_length + 1):(a_length +
b_length)] = b_cov

## Convert to asymptotic covariance matrix
asymp_reg_cov = n * joint_cov

## Pre- and post-multiply asymptotic covariance by gradient of transformation
med_asymp_var = grad_med %*% asymp_reg_cov %*% grad_med

## Get small-sample standard error
med_asymp_SE = sqrt(med_asymp_var)
med_SE = med_asymp_SE / sqrt(n)
```

Putting everything together, we have an estimated mediation effect of `r med_hat`, with an estimated SE of `r med_SE`.


































# Binary Response, Binary Mediator, Mixed-Effects

We now introduce random effects to our model. This changes many of the details, but the overall structure of the analysis remains the same. I still need to write some helper functions to compute relevant derivatives, all of which validated against a simple finite-difference approximation. See `Helpers.R`.

```{r}
source("../src/Helpers.R")

library(lme4)     # For fitting GLMMs using glmer()
library(MASS)     # For simulating multivariate normals using mvrnorm()
library(merDeriv) # For computing the full information matrix from models fit using lme4
```

## Generate Data

As in the previous case, we set intercepts so that the means of both linear predictors are at least approximately zero. We also need to set covariance parameters for the random effects. For this, we use either $20\%$ of the true value or $0.2$ if the true value is zero. Correlations between all random effects are set to $0.5$. Note that we include random effects for the intercept and the exposure, but not for the mediator.

Another difference from the previous settings is that we must now construct our dataset in clusters. We will use $K=5$ such clusters, with $n$ observations in each cluster. We will set $n=100$ for the first iteration of the loop.


```{r setup_mixed}
# Fixed effects for intercepts
a_0 = 0
b_0 = -0.5

# Random effects
sigma_a_0 = 0.2
sigma_a_1 = 0.2 * abs(a_1)
cor_a0_a1 = 0.2
cov_a0_a1 = sigma_a_0 * sigma_a_1 * cor_a0_a1
Sigma_a = matrix(c(sigma_a_0^2, cov_a0_a1, cov_a0_a1, sigma_a_1^2), nrow = 2, ncol = 2)

sigma_b_0 = 0.2 * abs(b_0)
sigma_b_1 = 0.2 * abs(b_1)
cor_b0_b1 = 0.2
cov_b0_b1 = sigma_b_0 * sigma_b_1 * cor_b0_b1
Sigma_b = matrix(c(sigma_b_0^2, cov_b0_b1, cov_b0_b1, sigma_b_1^2), nrow = 2, ncol = 2)

# Number of clusters
K=5
```

We first demonstrate our analysis on a single dataset, then put this analysis inside a `for` loop. The first step is to set the sample size and generate some data. We use $\eta$ to denote the linear predictor for $M$, and $\zeta$ for the linear predictor of $Y$. <!-- (This is also a comment) without the $M$ term. That is, $\zeta = b_0 + b_2 X + W B_3$. -->


```{r generate_data_mixed}
n = all_Ns[2]
# n = all_Ns[1]

all_Xs = list()
all_Ws = list()

for(k in 1:K){
  X = rnorm(n, mean=0, sd=1)
  W = matrix(rnorm(n*p_conf, mean=0, sd=1), nrow = n, ncol = p_conf)
  
  all_Xs[[k]] = X
  all_Ws[[k]] = W
}

# Generate M
all_Ms = list()
for(k in 1:K){
  eta_vec_fixed = a_0 + a_1*all_Xs[[k]] + all_Ws[[k]]%*%A_2
  
  ## Add random effects
  a_ran = mvrnorm(1, mu = rep(0, 2), Sigma = Sigma_a)
  eta_vec = eta_vec_fixed + a_ran[1] + a_ran[2]*all_Xs[[k]]
  
  ## Generate M
  p_M_vec = expit(eta_vec)
  M = rbinom(n, size = 1, prob = p_M_vec)
  all_Ms[[k]] = M
}

# Generate Y
all_Ys = list()
for(k in 1:K){
  zeta_vec_fixed = b_0 + b_1*all_Ms[[k]] + b_2 * all_Xs[[k]] + all_Ws[[k]]%*%B_3
  
  ## Add random effects
  b_ran = mvrnorm(1, mu = rep(0, 2), Sigma = Sigma_b)
  zeta_vec = zeta_vec_fixed + b_ran[1] + b_ran[2]*all_Xs[[k]]
  
  ## Generate Y
  p_Y_vec = expit(zeta_vec)
  Y = rbinom(n, size = 1, prob = p_Y_vec)
  all_Ys[[k]] = Y
}


# Consolidate groups
X = do.call(c, all_Xs)
W = do.call(rbind, all_Ws)
M = do.call(c, all_Ms)
Y = do.call(c, all_Ys)
group = rep(1:K, each = n)
```


## Estimate Mediation Effect

We now fit logistic regression models to predict $M$ and $Y$, and extract output that we will need later.

```{r fit_models_mixed}
M_data = data.frame(M, X, W1 = W[, 1], W2 = W[, 2], W3 = W[,3], group = group)
M_model = glmer(M ~ X + W1 + W2 + W3 + (1 + X | group), data = M_data, family = binomial(link = "logit"))
M_model_info = attributes(VarCorr(M_model)$group)

a_hat = fixef(M_model)
a_RE_sds = M_model_info$stddev
a_RE_cor = M_model_info$correlation[2,1]
theta_hat = c(a_RE_sds, a_RE_cor)
M_cov = vcov(M_model, full=TRUE, ranpar="sd")

# a_hat = summary(M_model)$coefficients[, 1]
# a_SE = summary(M_model)$coefficients[, 2]
# a_cov = vcov(M_model)


Y_data = data.frame(Y, M, X, W1 = W[, 1], W2 = W[, 2], W3 = W[,3], group = group)
Y_model = glmer(Y ~ M + X + W1 + W2 + W3 + (1 + X | group), data = Y_data, family = binomial(link = "logit"))
Y_model_info = attributes(VarCorr(Y_model)$group)

b_hat = fixef(Y_model)
b_RE_sds = Y_model_info$stddev
b_RE_cor = Y_model_info$correlation[2,1]
gamma_hat = c(b_RE_sds, b_RE_cor)
Y_cov = vcov(Y_model, full=TRUE, ranpar="sd")

# b_hat = summary(Y_model)$coefficients[, 1]
# b_SE = summary(Y_model)$coefficients[, 2]
# b_cov = vcov(Y_model)
```

Next, we estimate the mediation effect. Note that, as in the contiuous outcome, binary mediator setting, our mediation effect depends on $X$ and $W$. We again use $X=0$ and $W=[1,1,1]$, although Samoilenko and Lefebvre (2023) use the sample means of these covariates.

```{r}

# Fixed-effects
x_pred = 0
W_pred = c(1,1,1)

a_0_hat = a_hat[1]
a_x_hat = a_hat[2]
A_2_hat = a_hat[3:5]

b_0_hat = b_hat[1]
b_m_hat = b_hat[2]
b_x_hat = b_hat[3]
B_3_hat = b_hat[4:6]


# Linear predictors
eta_hat = as.numeric(a_0_hat + a_x_hat*x_pred + W_pred%*%A_2_hat)
zeta_hat = as.numeric(b_0_hat + b_x_hat * x_pred + W_pred %*% B_3_hat)


# Random effects covariances
s_M_0 = a_RE_sds[1]
s_M_x = a_RE_sds[2]
rho_M = a_RE_cor

s_Y_0 = b_RE_sds[1]
s_Y_x = b_RE_sds[2]
rho_Y = b_RE_cor


# Sigma functions
sigma_M1 = sigma_fun(x_pred, s_M_0, s_M_x, rho_M)
sigma_M2 = sigma_fun(x_pred + 1, s_M_x, s_M_0, rho_M)

sigma_Y1 = sigma_fun(x_pred, s_Y_0, s_Y_x, rho_Y)
sigma_Y2 = sigma_fun(x_pred + 1, s_Y_x, s_Y_0, rho_Y)

# Mediation effect
## See Helpers.R for the function Phi, which computes the mediation effect on odds-ratio scale
med_hat = Phi(eta_hat, zeta_hat, a_x_hat, b_m_hat, b_x_hat, sigma_M2, sigma_Y2, sigma_M1, sigma_Y1)
```


Applying the $\delta$-method here is a bit more complicated than in the other settings considered so far. Following the notes written by B & B, we take a two-step approach. We first transform model parameters from the two GLMMs into the vector of arguments to $\Phi$; call such a function $\xi$. We then compute the gradient of $\Phi$ with respect to these arguments, and use the $\delta$-method to get the standard error.

In order to perform the first step, we need the gradient of $\xi$. Fortunately, much of this has already been done for us. Specifically, B & B worked out the gradients of each of the four $\sigma$ functions. All that remains is to repeat this process for the other arguments of $\Phi$, and stack the results in a matrix. See the `Helpers.R` file for the relevant functions.



Starting first with 
We now compute the gradient of the odds ratio and use the $\delta$-method to get the standard error.

```{r}
# Gradient of OR wrt regression coefficients
grad_Phi_obs = grad_Phi(eta_hat, zeta_hat, a_x_hat, b_m_hat, b_x_hat, sigma_M2, sigma_Y2, sigma_M1, sigma_Y1)
grad_xi_obs = grad_xi(a_hat, theta_hat, b_hat, gamma_hat, x_pred, W_pred)
d_Phi_d_GLMM_pars = grad_Phi_obs %*% grad_xi_obs

# Get asymptotic SE using delta method

## Build joint covariance matrix of regression coefficients
M_length = nrow(M_cov)
Y_length = nrow(Y_cov)
joint_cov = matrix(0, nrow = M_length + Y_length, ncol = M_length +
Y_length)
joint_cov[1:M_length, 1:M_length] = M_cov
joint_cov[(M_length + 1):(M_length + Y_length), (M_length + 1):(M_length +
Y_length)] = Y_cov

## Convert to asymptotic covariance matrix
asymp_reg_cov = n * joint_cov

## Pre- and post-multiply asymptotic covariance by gradient of Phi wrt GLMM parameters
med_asymp_var = d_Phi_d_GLMM_pars %*% asymp_reg_cov %*% t(d_Phi_d_GLMM_pars)

## Get small-sample standard error
med_asymp_SE = sqrt(med_asymp_var)
med_SE = med_asymp_SE / sqrt(n)
```

Putting everything together, we have an estimated mediation effect of `r med_hat`, with an estimated SE of `r med_SE`.





















## Monte Carlo Study

We now repeat the above analysis `r num_reps` times for each of various values of $n$. Values of $X$ and $W$ for which we compute the total effect are set at the beginning. We also re-run the boilerplate code (not shown).


```{r, include=FALSE}
<<boilerplate>>


```

```{r}
source("../src/Helpers.R")

library(lme4)     # For fitting GLMMs using glmer()
library(MASS)     # For simulating multivariate normals using mvrnorm()
library(merDeriv) # For computing the full information matrix from models fit using lme4
library(tictoc)
library(ggplot2)
library(magrittr)
library(dplyr)

### Easier version of problem ###

all_Ns = c(50, 200, 500, 1000)  # Sample size for each cluster
all_Ks = c(5, 20, 50)           # Number of clusters
N_K_combs = expand.grid(N = all_Ns, K = all_Ks)

# Number of replicates for each sample size
num_reps = 200

# Values of X and W for which we compute the total effect
x_pred = 0
W_pred = c(1,1,1)

# Fixed effects for intercepts
a_0 = 0
a = c(a_0, a_1, A_2)
b_0 = -0.5
b = c(b_0, b_1, b_2, B_3)

# Random effects
sigma_a_0 = 0.5
sigma_a_1 = 0.5 * abs(a_1)
cor_a0_a1 = 0.5
cov_a0_a1 = sigma_a_0 * sigma_a_1 * cor_a0_a1
theta = c(sigma_a_0, sigma_a_1, cor_a0_a1)
Sigma_a = matrix(c(sigma_a_0^2, cov_a0_a1, cov_a0_a1, sigma_a_1^2), nrow = 2, ncol = 2)

sigma_b_0 = 0.5 * abs(b_0)
sigma_b_1 = 0.5 * abs(b_1)
cor_b0_b1 = 0.5
cov_b0_b1 = sigma_b_0 * sigma_b_1 * cor_b0_b1
gamma = c(sigma_b_0, sigma_b_1, cor_b0_b1)
Sigma_b = matrix(c(sigma_b_0^2, cov_b0_b1, cov_b0_b1, sigma_b_1^2), nrow = 2, ncol = 2)




### Harder version ###

# # Fewer large sample sizes
# # all_Ns = c(100, 200, 500, 1000, 2000)
# # all_Ns = c(200, 500, 1000, 2000)
# # all_Ns = c(200, 500, 1000)
# # all_Ns = c(200, 500, 1000)
# 
# Values of X and W for which we compute the total effect
# x_pred = 0
# W_pred = c(1,1,1)
# 
# # Fixed effects for intercepts
# a_0 = 0
# a = c(a_0, a_1, A_2)
# b_0 = -0.5
# b = c(b_0, b_1, b_2, B_3)
# 
# # # Random effects
# # sigma_a_0 = 0.2
# # sigma_a_1 = 0.2 * abs(a_1)
# # cor_a0_a1 = 0.2
# # cov_a0_a1 = sigma_a_0 * sigma_a_1 * cor_a0_a1
# # theta = c(sigma_a_0, sigma_a_1, cor_a0_a1)
# # Sigma_a = matrix(c(sigma_a_0^2, cov_a0_a1, cov_a0_a1, sigma_a_1^2), nrow = 2, ncol = 2)
# # 
# # sigma_b_0 = 0.2 * abs(b_0)
# # sigma_b_1 = 0.2 * abs(b_1)
# # cor_b0_b1 = 0.2
# # cov_b0_b1 = sigma_b_0 * sigma_b_1 * cor_b0_b1
# # gamma = c(sigma_b_0, sigma_b_1, cor_b0_b1)
# # Sigma_b = matrix(c(sigma_b_0^2, cov_b0_b1, cov_b0_b1, sigma_b_1^2), nrow = 2, ncol = 2)
# 
# # Number of clusters
# # all_Ks = c(5, 20, 50, 100)
# # K=5
# 
# # N_K_combs = expand.grid(N = all_Ns, K = all_Ks)
# 
# # Number of replicates for each sample size
# # We use fewer here because model fitting takes longer
# num_reps = 200





```


This simulation takes a while. As such, I have saved the results and don't re-run things every time I compile this document. To re-simulate, un-comment the following code chunk.

```{r MC-bin-bin-mix, include=FALSE}
# , dependson=c("boilerplate")}

# Commented out to avoid lengthy runtime

library(doParallel)
library(pbapply)

# tic()
# 
# set.seed(1)
# 
# # Containers for output
# all_a_hats = list()
# all_theta_hats = list()
# all_b_hats = list()
# all_gamma_hats = list()
# 
# all_M_covs = list()
# all_Y_covs = list()
# 
# all_med_hats = list()
# all_med_SEs = list()
# 
# 
# # Initialize cluster on my machine ----
# # n_cores = 2
# n_cores = 10
# # n_cores = parallel::detectCores() - 1
# my_cluster = makeCluster(n_cores, type = "PSOCK")
# registerDoParallel(my_cluster)
# clusterEvalQ(my_cluster,{
#   library(MASS)
#   library(lme4)
#   library(merDeriv)
#   source("../src/Helpers.R")
# })
# clusterExport(my_cluster, c("N_K_combs", "num_reps", "a_0", "a_1", "A_2", "a", "b_0", "b_1", "b_2", "B_3", "b", "Sigma_a", "theta", "Sigma_b", "gamma", "p_conf", "x_pred", "W_pred"))
# clusterSetRNGStream(my_cluster, iseed = 1)
# 
# # output_bin_bin_ran = pblapply(seq_len(nrow(N_K_combs)), function(j){
# # output_bin_bin_ran = pblapply(nrow(N_K_combs), function(j){
# for(j in seq_len(nrow(N_K_combs))){
#   n = N_K_combs[j, "N"]
#   K = N_K_combs[j, "K"]
# 
#   clusterExport(my_cluster, c("n", "K"))
# 
#   print(paste0("j = ", j, " of ", nrow(N_K_combs),  " (n=", n, ", K=", K, ")"))
# 
# 
#   # # Containers for output with this value of n
#   # some_a_hats = list()
#   # some_theta_hats = list()
#   # some_b_hats = list()
#   # some_gamma_hats = list()
#   #
#   # some_M_covs = list()
#   # some_Y_covs = list()
#   #
#   # some_med_hats = list()
#   # some_med_SEs = list()
# 
#   # for(i in 1:num_reps){
#   some_output = pblapply(seq_len(num_reps), function(i){
#     # print(paste0("n = ", n, ", K = ", K, ", i = ", i, " of ", num_reps))
# 
#     this_output = NULL
# 
#     # Use tryCatch to omit any datasets leading to convergence issues
#     tryCatch({                                                                    ############## Un-quote
#       all_Xs = list()
#       all_Ws = list()
# 
#       for(k in 1:K){
#         X = rnorm(n, mean=0, sd=1)
#         W = matrix(rnorm(n*p_conf, mean=0, sd=1), nrow = n, ncol = p_conf)
# 
#         all_Xs[[k]] = X
#         all_Ws[[k]] = W
#       }
# 
#       # Generate M
#       all_Ms = list()
#       for(k in 1:K){
#         eta_vec_fixed = a_0 + a_1*all_Xs[[k]] + all_Ws[[k]]%*%A_2
# 
#         ## Add random effects
#         a_ran = mvrnorm(1, mu = rep(0, 2), Sigma = Sigma_a)
#         eta_vec = eta_vec_fixed + a_ran[1] + a_ran[2]*all_Xs[[k]]
# 
#         ## Generate M
#         p_M_vec = expit(eta_vec)
#         M = rbinom(n, size = 1, prob = p_M_vec)
#         all_Ms[[k]] = M
#       }
# 
#       # Generate Y
#       all_Ys = list()
#       for(k in 1:K){
#         zeta_vec_fixed = b_0 + b_1*all_Ms[[k]] + b_2 * all_Xs[[k]] + all_Ws[[k]]%*%B_3
# 
#         ## Add random effects
#         b_ran = mvrnorm(1, mu = rep(0, 2), Sigma = Sigma_b)
#         zeta_vec = zeta_vec_fixed + b_ran[1] + b_ran[2]*all_Xs[[k]]
# 
#         ## Generate Y
#         p_Y_vec = expit(zeta_vec)
#         Y = rbinom(n, size = 1, prob = p_Y_vec)
#         all_Ys[[k]] = Y
#       }
# 
# 
#       # Consolidate groups
#       X = do.call(c, all_Xs)
#       W = do.call(rbind, all_Ws)
#       M = do.call(c, all_Ms)
#       Y = do.call(c, all_Ys)
#       group = rep(1:K, each = n)
# 
# 
#       # Fit models
# 
#       ## M
#       M_data = data.frame(M, X, W1 = W[, 1], W2 = W[, 2], W3 = W[,3], group = group)
#       M_model = glmer(M ~ X + W1 + W2 + W3 + (1 + X | group), data = M_data, family = binomial(link = "logit"))
#       M_model_info = attributes(VarCorr(M_model)$group)
# 
#       ### Fitted parameters
#       a_hat = fixef(M_model)
#       a_RE_sds = M_model_info$stddev
#       a_RE_cor = M_model_info$correlation[2,1]
#       theta_hat = c(a_RE_sds, a_RE_cor)
#       if(any(is.nan(theta_hat))) stop("NaNs in theta_hat")  # Skip rest of current analysis if correlation is 0/0
# 
#       ### Estimated SE covariance matrix
#       M_cov = vcov(M_model, full=TRUE, ranpar="sd")
# 
#       # some_a_hats[[i]] = a_hat
#       # some_theta_hats[[i]] = theta_hat
#       # some_M_covs[[i]] = M_cov
# 
# 
#       ## Y
#       Y_data = data.frame(Y, M, X, W1 = W[, 1], W2 = W[, 2], W3 = W[,3], group = group)
#       Y_model = glmer(Y ~ M + X + W1 + W2 + W3 + (1 + X | group), data = Y_data, family = binomial(link = "logit"))
#       Y_model_info = attributes(VarCorr(Y_model)$group)
# 
#       ### Fitted parameters
#       b_hat = fixef(Y_model)
#       b_RE_sds = Y_model_info$stddev
#       b_RE_cor = Y_model_info$correlation[2,1]
#       gamma_hat = c(b_RE_sds, b_RE_cor)
#       if(any(is.nan(gamma_hat))) stop("NaNs in gamma_hat")  # Skip rest of current analysis if correlation is 0/0
# 
#       ### Estimated SE covariance matrix
#       Y_cov = vcov(Y_model, full=TRUE, ranpar="sd")
# 
#       # some_b_hats[[i]] = b_hat
#       # some_gamma_hats[[i]] = gamma_hat
#       # some_Y_covs[[i]] = Y_cov
# 
# 
# 
#       # Estimate mediation effect
# 
#       ## Fixed-effects
# 
#       a_0_hat = a_hat[1]
#       a_x_hat = a_hat[2]
#       A_2_hat = a_hat[3:5]
# 
#       b_0_hat = b_hat[1]
#       b_m_hat = b_hat[2]
#       b_x_hat = b_hat[3]
#       B_3_hat = b_hat[4:6]
# 
# 
#       ## Linear predictors
#       eta_hat = as.numeric(a_0_hat + a_x_hat*x_pred + W_pred%*%A_2_hat)
#       zeta_hat = as.numeric(b_0_hat + b_x_hat * x_pred + W_pred %*% B_3_hat)
# 
# 
#       ## Random effects covariances
#       s_M_0 = a_RE_sds[1]
#       s_M_x = a_RE_sds[2]
#       rho_M = a_RE_cor
# 
#       s_Y_0 = b_RE_sds[1]
#       s_Y_x = b_RE_sds[2]
#       rho_Y = b_RE_cor
# 
# 
#       ## Sigma functions
#       sigma_M1 = sigma_fun(x_pred, s_M_0, s_M_x, rho_M)
#       sigma_M2 = sigma_fun(x_pred + 1, s_M_x, s_M_0, rho_M)
# 
#       sigma_Y1 = sigma_fun(x_pred, s_Y_0, s_Y_x, rho_Y)
#       sigma_Y2 = sigma_fun(x_pred + 1, s_Y_x, s_Y_0, rho_Y)
# 
#       ## Mediation effect
#       ### See Helpers.R for the function Phi, which computes the mediation effect on odds-ratio scale
#       med_hat = Phi(eta_hat, zeta_hat, a_x_hat, b_m_hat, b_x_hat, sigma_M2, sigma_Y2, sigma_M1, sigma_Y1)
#       # some_med_hats[[i]] = med_hat
# 
# 
# 
# 
# 
#       # Estimate SE
# 
#       ## Gradient of OR wrt regression coefficients
#       grad_Phi_obs = grad_Phi(eta_hat, zeta_hat, a_x_hat, b_m_hat, b_x_hat, sigma_M2, sigma_Y2, sigma_M1, sigma_Y1)
#       grad_xi_obs = grad_xi(a_hat, theta_hat, b_hat, gamma_hat, x_pred, W_pred)
#       d_Phi_d_GLMM_pars = grad_Phi_obs %*% grad_xi_obs
# 
#       ## Get asymptotic SE using delta method
# 
#       ### Build joint covariance matrix of regression coefficients
#       M_length = nrow(M_cov)
#       Y_length = nrow(Y_cov)
#       joint_cov = matrix(0, nrow = M_length + Y_length, ncol = M_length +
#       Y_length)
#       joint_cov[1:M_length, 1:M_length] = M_cov
#       joint_cov[(M_length + 1):(M_length + Y_length), (M_length + 1):(M_length +
#       Y_length)] = Y_cov
# 
#       ### Convert to asymptotic covariance matrix
#       asymp_reg_cov = n * joint_cov
# 
#       ### Pre- and post-multiply asymptotic covariance by gradient of Phi wrt GLMM parameters
#       med_asymp_var = d_Phi_d_GLMM_pars %*% asymp_reg_cov %*% t(d_Phi_d_GLMM_pars)
# 
#       ### Get small-sample standard error
#       med_asymp_SE = sqrt(med_asymp_var)
#       med_SE = med_asymp_SE / sqrt(n)
# 
#       # some_med_SEs[[i]] = med_SE
# 
#       this_output = list(a_hat = a_hat, theta_hat = theta_hat, b_hat = b_hat, gamma_hat = gamma_hat, M_cov = M_cov, Y_cov = Y_cov, med_hat = med_hat, med_SE = med_SE)
#     # }, error = function(e){grad_Phi_obs <<- e}) # For troubleshooting parallel errors
#     }, error = function(e){})                                                       ############ Un-quote
# 
#     return(this_output)
# 
# 
#   }, cl = my_cluster)
#   # })
# 
#   # Store output for current values of n and K
# 
#   all_a_hats[[j]] = lapply(some_output, function(x) x$a_hat)
#   all_theta_hats[[j]] = lapply(some_output, function(x) x$theta_hat)
#   all_b_hats[[j]] = lapply(some_output, function(x) x$b_hat)
#   all_gamma_hats[[j]] = lapply(some_output, function(x) x$gamma_hat)
#   all_M_covs[[j]] = lapply(some_output, function(x) x$M_cov)
#   all_Y_covs[[j]] = lapply(some_output, function(x) x$Y_cov)
#   all_med_hats[[j]] = lapply(some_output, function(x) x$med_hat)
#   all_med_SEs[[j]] = lapply(some_output, function(x) x$med_SE)
# }
# 
# 
# runtime = toc()
# 
# parallel::stopCluster(my_cluster)


# Save output
# save(N_K_combs, all_a_hats, all_theta_hats, all_b_hats, all_gamma_hats, all_M_covs, all_Y_covs, all_med_hats, all_med_SEs, runtime, file = "William-Analysis_cache/MC-bin-bin-mix.RData")
load("William-Analysis_cache/MC-bin-bin-mix.RData", verbose = TRUE)
```

We now process the output from our simulation and summarize the results in a table. We give the mean and median of the estimated standard errors, as well as their relative errors (in $\%$) as estimates of the empirical standard error. See Table \ref{tab:results_bin_bin_mix}.

```{r}
# Re-format output to be more useful. 
data_med_hats = lapply(all_med_hats, unlist)
data_med_SEs = all_med_SEs %>% lapply(unlist) %>% lapply(na.omit)

# SD of estimates across Monte Carlo samples
emp_SEs = sapply(data_med_hats, sd)

# Summaries of estimated SEs (mean and median)
mean_delta_SEs = sapply(data_med_SEs, mean)
median_delta_SEs = sapply(data_med_SEs, median)

# Combine into a table
results = data.frame(n = N_K_combs$N, K = N_K_combs$K, Empirical = emp_SEs,
                     Mean = mean_delta_SEs, Median = median_delta_SEs,
                     "Percent_Err_Mean" = 100*(mean_delta_SEs - emp_SEs)/emp_SEs,
                     "Percent_Err_Median" = 100*(median_delta_SEs - emp_SEs)/emp_SEs)


# Add extra values of n and K to list
# all_N_combs = c(N_K_combs$N)
# all_K_combs = c(N_K_combs$K)
# results = data.frame(n = all_N_combs, K = all_K_combs, Empirical = emp_SEs,
#                      Mean = mean_delta_SEs, Median = median_delta_SEs,
#                      "Percent_Err_Mean" = 100*(mean_delta_SEs - emp_SEs)/emp_SEs,
#                      "Percent_Err_Median" = 100*(median_delta_SEs - emp_SEs)/emp_SEs)

```



```{r results="asis", echo=FALSE}
## Print table
kbl(results, format = "latex", booktabs = T, caption = "Summary of SE estimates for mediation effect under continuous response, continuous mediator, fixed-effects model \\label{tab:results_bin_bin_mix}")
```



There's lots that I don't understand about the above results. One think I decided to explore is the normality of the estimators. To this end, I make some density plots, and also perform Shapiro-Wilk tests for normality. See Table \ref{tab:results_bin_bin_mix-Shapiro} for the test results.

```{r}
med_hats_to_plot = unlist(data_med_hats)
Ns_to_plot = rep(N_K_combs$N, times = sapply(data_med_hats, length))
Ks_to_plot = rep(N_K_combs$K, times = sapply(data_med_hats, length))
data_med_hat_to_plot = data.frame(med_hat = med_hats_to_plot, N = Ns_to_plot, K = Ks_to_plot)

data_med_hat_to_plot_OLD = data_med_hat_to_plot
data_med_hat_to_plot %<>% dplyr::filter(med_hat < 4)
```


```{r}
ggplot(data_med_hat_to_plot, aes(x = med_hat)) +
  geom_density() +
  facet_grid(rows = vars(N), cols = vars(K), scales = "free") +
  ggtitle("Density of mediation effect estimates")
  # theme_minimal()
```


```{r}
# QQ plot for each sample size
ggplot(data_med_hat_to_plot, aes(sample = med_hat)) +
  stat_qq() +
  stat_qq_line() +
  facet_grid(rows = vars(N), cols = vars(K), scales = "free") +
  ggtitle("QQ plot of mediation effect estimates")
  # theme_minimal()
```


```{r}
# Test normality in each sample size
all_norm_p_vals = c()
for(i in 1:nrow(N_K_combs)){
  p_val = shapiro.test(data_med_hats[[i]])$p.value
  all_norm_p_vals = c(all_norm_p_vals, p_val)
}

results$Shapiro_p = all_norm_p_vals
# results

# # Check how p-value varies with N and K
# p_vals_by_N = results %>% group_by(n) %>% summarize(mean_p = mean(Shapiro_p), sd_p = sd(Shapiro_p))
# p_vals_by_K = results %>% group_by(K) %>% summarize(mean_p = mean(Shapiro_p), sd_p = sd(Shapiro_p))

```

```{r results="asis", echo=FALSE}
## Print table
kbl(results, format = "latex", booktabs = T, caption = "Summary of SE estimates for mediation effect under continuous response, continuous mediator, fixed-effects model \\label{tab:results_bin_bin_mix-Shapiro}")
```
